---
title: '2. Set the policies and thresholds for detecting harmful content'
layout: default
nav_order: 2
parent: 'Exercise 02: Demo Defender with content safety contextual alert from AOAI'
---

## Task 2: Set the policies and thresholds for detecting harmful content 


1. On the **ContentSafetyXXXX** blade, in the navigation pane, select **Overview**.

1. Scroll-down and under **Make an API call**, select **Content Safety Studio**.

1. The Content Safety Studio will open in a new tab, in the upper-right corner, select **Sign In**.

1. On the Pick an account window, select your lab admin user account **@lab.CloudPortalCredential(User1).Username**.

1. Back in Content Safety Studio, select **Moderate text content**.

1. On the **Moderate text content** blade, under **Try it Out**, verify the checkbox is selected and then select **Choose a different resource**.

1. On the **Settings** blade, select **ContentSafetyXXXX** and then select **Use resource**.

1. Back on the **Moderate text content** blade, under **1. Select a sample or type your own**, select **Violent content with misspelling**.

1. Under the **2. Test** text box, select **Run test**.

1. Under **3. View results**, verify the content has been **Blocked**, and has been rejected using the **Violence** category.

1. Also notice the other categories that were judged to be **Allowed**.

1. In the upper-left corner, select **Content Safety Studio** to return to the main page.

1. Scroll down and then select **Prompt Shields**.

1. On the **Prompt shields** blade, under **1. Select a sample or type your own**, select **Document attack content**.

1. Under the **2. Test** text box, select **Run test**.

1. Under **3. View results**, verify the results show that jailbreak attacks were detected in the document.